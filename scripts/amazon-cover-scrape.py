#!/usr/bin/env python3
"""
Script pour scraper les couvertures de livres depuis Amazon.fr
Utilise les URLs du fichier bibliotheque.json et sauvegarde les images dans img/books/

NÃ©cessite: pip install requests beautifulsoup4 lxml pillow
"""

import json
import os
import sys
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from pathlib import Path

# Configuration
BASE_DIR = Path(__file__).parent.parent
JSON_PATH = BASE_DIR / "data" / "bibliotheque.json"
IMG_DIR = BASE_DIR / "img" / "books"

# Headers pour simuler un navigateur
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate, br',
    'DNT': '1',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

def load_books():
    """Charge la liste des livres depuis le fichier JSON"""
    try:
        with open(JSON_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('livres', [])
    except FileNotFoundError:
        print(f"âŒ Fichier non trouvÃ©: {JSON_PATH}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ Erreur de lecture JSON: {e}")
        sys.exit(1)

def get_cover_url_from_amazon(url, expected_title, expected_author):
    """
    Scrape la page Amazon pour rÃ©cupÃ©rer l'URL de la couverture
    VÃ©rifie la cohÃ©rence du livre avec le titre et l'auteur attendus
    """
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'lxml')

        # VÃ©rification : extraire le titre de la page
        page_title = None
        title_selectors = [
            '#productTitle',
            'h1.a-size-large',
            'span#ebooksProductTitle',
        ]

        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                page_title = title_elem.get_text(strip=True)
                break

        # VÃ©rification : extraire l'auteur de la page
        page_author = None
        author_selectors = [
            '.author .contributorNameID',
            '.author a.a-link-normal',
            'span.author a',
            '#bylineInfo .author a',
        ]

        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                page_author = author_elem.get_text(strip=True)
                break

        # Comparer avec les valeurs attendues
        title_match = False
        author_match = False

        if page_title:
            # Normalisation pour comparaison souple
            normalized_page_title = page_title.lower().strip()
            normalized_expected = expected_title.lower().strip()

            # VÃ©rifier si le titre contient ou est contenu dans l'attendu
            title_match = (normalized_expected in normalized_page_title or
                          normalized_page_title in normalized_expected)

        if page_author:
            normalized_page_author = page_author.lower().strip()
            normalized_expected_author = expected_author.lower().strip()

            # VÃ©rifier si l'auteur correspond
            author_match = (normalized_expected_author in normalized_page_author or
                           normalized_page_author in normalized_expected_author)

        # Si on a trouvÃ© un titre mais qu'il ne correspond pas
        if page_title and not title_match:
            print(f"  âš ï¸  ATTENTION : Titre incohÃ©rent!")
            print(f"      Attendu : '{expected_title}'")
            print(f"      TrouvÃ©  : '{page_title}'")
            return None, "ERREUR_TITRE"

        # Si on a trouvÃ© un auteur mais qu'il ne correspond pas
        if page_author and not author_match:
            print(f"  âš ï¸  ATTENTION : Auteur incohÃ©rent!")
            print(f"      Attendu : '{expected_author}'")
            print(f"      TrouvÃ©  : '{page_author}'")
            return None, "ERREUR_AUTEUR"

        # Plusieurs sÃ©lecteurs possibles pour trouver l'image de couverture
        selectors = [
            '#imgBlkFront',  # Image principale du livre
            '#ebooksImgBlkFront',  # Pour les ebooks
            '#main-image',
            'img[data-a-dynamic-image]',  # Image avec data dynamique
            '.a-dynamic-image',
        ]

        cover_url = None

        for selector in selectors:
            img = soup.select_one(selector)
            if img:
                # Essayer diffÃ©rents attributs
                cover_url = img.get('src') or img.get('data-old-hires') or img.get('data-a-dynamic-image')

                # Si c'est un JSON dans data-a-dynamic-image
                if cover_url and cover_url.startswith('{'):
                    try:
                        images = json.loads(cover_url)
                        # Prendre la plus grande image
                        cover_url = max(images.keys(), key=lambda k: images[k][0] * images[k][1])
                    except:
                        pass

                if cover_url:
                    # Nettoyer l'URL (enlever les paramÃ¨tres de taille)
                    if '._' in cover_url:
                        cover_url = cover_url.split('._')[0] + '.jpg'
                    break

        if not cover_url:
            # Chercher dans toutes les images
            all_imgs = soup.find_all('img', {'class': lambda x: x and 'bookImage' in x or 'imageBlock' in x})
            for img in all_imgs:
                src = img.get('src', '')
                if 'images-amazon.com/images/I/' in src:
                    cover_url = src
                    break

        if cover_url:
            print(f"  âœ“ VÃ©rifications OK - Titre: {page_title or 'N/A'}")
            print(f"                     Auteur: {page_author or 'N/A'}")

        return cover_url, None

    except requests.RequestException as e:
        print(f"  âš ï¸  Erreur de requÃªte: {e}")
        return None, "ERREUR_RESEAU"
    except Exception as e:
        print(f"  âš ï¸  Erreur inattendue: {e}")
        return None, "ERREUR_INCONNUE"

def download_image(url, output_path):
    """TÃ©lÃ©charge une image depuis une URL"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10, stream=True)
        response.raise_for_status()

        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        return True
    except Exception as e:
        print(f"  âŒ Ã‰chec du tÃ©lÃ©chargement: {e}")
        return False

def main():
    print("ğŸ” Scraping des couvertures Amazon.fr...\n")

    # CrÃ©er le dossier de destination
    IMG_DIR.mkdir(parents=True, exist_ok=True)

    # Charger les livres
    books = load_books()
    print(f"ğŸ“š {len(books)} livres trouvÃ©s dans {JSON_PATH}\n")

    success_count = 0
    skip_count = 0
    fail_count = 0
    error_count = 0
    errors = []

    for i, book in enumerate(books, 1):
        titre = book.get('titre', 'Sans titre')
        auteur = book.get('auteur', 'Auteur inconnu')
        image = book.get('image', '')
        url_amazon = book.get('url_amazon', '')

        if not url_amazon:
            print(f"{i}. â­ï¸  {titre} - Pas d'URL Amazon")
            skip_count += 1
            continue

        # Extraire le nom du fichier depuis le champ image
        if image:
            filename = Path(image).name
            output_path = IMG_DIR / filename
        else:
            print(f"{i}. âš ï¸  {titre} - Pas de nom de fichier image dÃ©fini")
            skip_count += 1
            continue

        # VÃ©rifier si l'image existe dÃ©jÃ 
        if output_path.exists():
            file_size = output_path.stat().st_size
            # Si le fichier fait plus de 5KB, on considÃ¨re qu'il est valide
            if file_size > 5000:
                print(f"{i}. âœ“ {titre} - Image dÃ©jÃ  prÃ©sente ({file_size // 1024}KB)")
                success_count += 1
                continue
            else:
                print(f"{i}. ğŸ”„ {titre} - Image trop petite ({file_size}B), re-tÃ©lÃ©chargement...")

        print(f"{i}. ğŸ“¥ {titre}")
        print(f"    Auteur: {auteur}")
        print(f"    URL: {url_amazon}")

        # Scraper l'URL de la couverture avec vÃ©rification
        cover_url, error_code = get_cover_url_from_amazon(url_amazon, titre, auteur)

        if error_code:
            print(f"  âŒ Erreur: {error_code}")
            errors.append({
                'titre': titre,
                'auteur': auteur,
                'url': url_amazon,
                'erreur': error_code
            })
            error_count += 1
            fail_count += 1
            time.sleep(2)
            continue

        if not cover_url:
            print(f"  âŒ Impossible de trouver l'image de couverture")
            fail_count += 1
            time.sleep(2)
            continue

        print(f"    Image trouvÃ©e: {cover_url[:80]}...")

        # TÃ©lÃ©charger l'image
        if download_image(cover_url, output_path):
            file_size = output_path.stat().st_size
            print(f"  âœ… TÃ©lÃ©chargÃ©: {filename} ({file_size // 1024}KB)")
            success_count += 1
        else:
            fail_count += 1

        # DÃ©lai entre les requÃªtes pour ne pas surcharger Amazon
        time.sleep(2)

    print("\n" + "="*60)
    print(f"ğŸ“Š RÃ©sumÃ©:")
    print(f"  âœ… SuccÃ¨s: {success_count}")
    print(f"  â­ï¸  IgnorÃ©s: {skip_count}")
    print(f"  âŒ Ã‰checs: {fail_count}")
    if error_count > 0:
        print(f"  âš ï¸  Erreurs de cohÃ©rence: {error_count}")
    print(f"  ğŸ“ Images dans: {IMG_DIR}")
    print("="*60)

    # Afficher le dÃ©tail des erreurs de cohÃ©rence
    if errors:
        print("\nâš ï¸  ERREURS DE COHÃ‰RENCE DÃ‰TECTÃ‰ES:")
        print("="*60)
        for err in errors:
            print(f"\nâŒ {err['titre']}")
            print(f"   Auteur: {err['auteur']}")
            print(f"   URL: {err['url']}")
            print(f"   Erreur: {err['erreur']}")
        print("\n" + "="*60)
        print("âš ï¸  Veuillez vÃ©rifier et corriger les URLs dans bibliotheque.json")
        print("="*60)

if __name__ == "__main__":
    main()
